{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self \n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = preprocessing.LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = preprocessing.LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7828947368421053"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('messidor_features.arff', delimiter=',', comments = '@', max_rows = 1000)\n",
    "X_train = data[:,:19]\n",
    "Y_train = data[:,19]\n",
    "# print(X_train[-1])\n",
    "# print(Y_train.shape)\n",
    "data = np.loadtxt('messidor_features.arff', delimiter=',', comments = '@', max_rows = 1000, skiprows=1023)\n",
    "X_test = data[:,:19]\n",
    "Y_test = data[:,19]\n",
    "# print(X_test[1])\n",
    "# print(Y_test.shape)\n",
    "logistic_model = sklearn.linear_model.LogisticRegression(C = 35,fit_intercept=False, penalty='l1', solver='liblinear',max_iter = 1000)\n",
    "logistic_model.fit(X_train, Y_train);\n",
    "Y_pred = logistic_model.predict(X_test)\n",
    "sklearn.metrics.accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '*\\x00\\x00\\x0c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-408829c85503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfilecp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'default of credit card clients.xls'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cp1252'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilecp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2750\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mY_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0;31m# converting the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;31m# Convert each value according to its column and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;31m# Convert each value according to its column and store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;31m# Then pack it according to the dtype's nesting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mfloatconv\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'0x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromhex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '*\\x00\\x00\\x0c'"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "\n",
    "filecp = codecs.open('default of credit card clients.xls', encoding = 'cp1252')\n",
    "data = np.loadtxt(filecp, delimiter=',',skiprows=2, max_rows = 2750)\n",
    "X_train = data[:,1:24]\n",
    "Y_train = data[:,24]\n",
    "print(X_train[1])\n",
    "print(X_train[-1])\n",
    "print(Y_train.shape)\n",
    "data = np.loadtxt('default of credit card clients.xls', delimiter=',', skiprows=2751)\n",
    "X_test = data[:,1:24]\n",
    "Y_test = data[:,24]\n",
    "print(X_test[1])\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000025       5       1       1       1       2       1       3       1\n",
      "       1]\n",
      "[1334659       5       2       4       1       1       1       1       1\n",
      "       1]\n",
      "['2' '2' '2' '2' '2' '4' '2' '2' '2' '2' '2' '2' '4' '2' '4' '4' '2' '2'\n",
      " '4' '2' '4' '4' '2' '4' '2' '4' '2' '2' '2' '2' '2' '2' '4' '2' '2' '2'\n",
      " '4' '2' '4' '4' '2' '4' '4' '4' '4' '2' '4' '2' '2' '4' '4' '4' '4' '4'\n",
      " '4' '4' '4' '4' '4' '4' '4' '2' '4' '4' '2' '4' '2' '4' '4' '2' '2' '4'\n",
      " '2' '4' '4' '2' '2' '2' '2' '2' '2' '2' '2' '2' '4' '4' '4' '4' '2' '2'\n",
      " '2' '2' '2' '2' '2' '2' '2' '2' '4' '4' '4' '4' '2' '4' '4' '4' '4' '4'\n",
      " '2' '4' '2' '4' '4' '4' '2' '2' '2' '4' '2' '2' '2' '2' '4' '4' '4' '2'\n",
      " '4' '2' '4' '2' '2' '2' '4' '2' '2' '2' '2' '2' '2' '2' '2' '2' '4' '2'\n",
      " '2' '2' '4' '2' '2' '4' '2' '4' '4' '2' '2' '4' '2' '2' '2' '4' '4' '2'\n",
      " '2' '2' '2' '2' '4' '4' '2' '2' '2' '2' '2' '4' '4' '4' '2' '4' '2' '4'\n",
      " '2' '2' '2' '4' '4' '2' '4' '4' '4' '2' '4' '4' '2' '2' '2' '2' '2' '2'\n",
      " '2' '2' '4' '4' '2' '2' '2' '4' '4' '2' '2' '2' '4' '4' '2' '4' '4' '4'\n",
      " '2' '2' '4' '2' '2' '4' '4' '4' '4' '2' '4' '4' '2' '4' '4' '4' '2' '4'\n",
      " '2' '2' '4' '4' '4' '4' '2' '2' '2' '2' '2' '2' '4' '4' '2' '2' '2' '4'\n",
      " '2' '4' '4' '4' '2' '2' '2' '2' '4' '4' '4' '4' '4' '2' '4' '4' '4' '2'\n",
      " '4' '2' '4' '4' '2' '2' '2' '2' '2' '4' '2' '2' '4' '4' '4' '4' '4' '2'\n",
      " '4' '4' '2' '2' '4' '4' '2' '4' '2' '2' '2' '4' '4' '2' '4' '2' '4' '4'\n",
      " '2' '2' '4' '2' '2' '2' '4' '2' '2' '2' '4' '4' '2' '2' '4' '2' '2' '4'\n",
      " '2' '2' '4' '2' '4' '4' '4' '2' '2' '4' '4' '2' '4' '2' '2' '4' '4' '2'\n",
      " '2' '2' '4' '2' '2' '2' '4' '4' '2' '2' '2' '4' '2' '2' '4' '4' '4' '4'\n",
      " '4' '4' '2' '2' '2' '2' '4' '4' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2'\n",
      " '2' '2' '2' '4' '2' '2' '2' '2' '4' '2' '2' '2' '2' '4' '2' '2' '2' '2'\n",
      " '2' '2' '2' '2' '4' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '4' '2'\n",
      " '4' '2' '4' '2' '2' '2' '2' '4' '2' '2' '2' '4' '2' '4' '2' '2' '2' '2'\n",
      " '2' '2' '2' '4' '4' '2' '2' '2' '4' '2' '2' '2' '2' '2' '2' '2' '2' '4'\n",
      " '2' '2' '2' '4' '2' '4' '4' '4' '2' '2' '2' '2' '2' '2' '2' '4' '4' '4'\n",
      " '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '4' '2' '2' '4' '4' '2' '2'\n",
      " '2' '4' '4' '4' '2' '4' '2' '4' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2'\n",
      " '2' '2' '4' '2' '2' '2' '2' '2' '2' '2' '4' '4' '2' '2' '2' '4' '2' '2'\n",
      " '4' '4' '2' '2' '2' '2' '2' '2' '4' '2' '2' '2' '2' '2' '2' '2' '2' '2'\n",
      " '2' '2' '2' '2' '2' '2' '4' '2' '2' '4' '2' '2' '2' '2' '2' '2' '2' '2'\n",
      " '2' '2' '2' '2' '2' '2' '2' '4' '2' '2' '4' '4' '4' '4' '2' '2' '4' '2'\n",
      " '2' '2' '2' '2' '2' '4' '4' '2' '2' '2' '4' '2' '4' '2' '4' '4' '4' '2'\n",
      " '4' '2' '2' '2' '2' '2']\n",
      "[1057067       1       1       1       1       1       0       1       1\n",
      "       1]\n",
      "(100,)\n",
      "['2' '2' '2' '2' '2' '4' '4' '4' '2' '4' '2' '4' '4' '4' '2' '2' '2' '2'\n",
      " '2' '2' '2' '2' '2' '2' '2' '2' '2' '4' '2' '2' '2' '2' '2' '2' '4' '2'\n",
      " '2' '4' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '4' '2' '2' '2' '2'\n",
      " '2' '2' '2' '2' '2' '4' '2' '2' '2' '2' '2' '2' '2' '2' '2' '2' '4' '4'\n",
      " '2' '2' '2' '2' '2' '2' '2' '2' '2' '4' '4' '2' '2' '2' '2' '2' '2' '2'\n",
      " '2' '2' '4' '2' '2' '2' '2' '4' '4' '4']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('breast-cancer-wisconsin.data',dtype = 'str', delimiter=',', max_rows = 600)\n",
    "X_train = data[:,:10]\n",
    "Y_train = data[:,10]\n",
    "X_train[X_train == '?'] = 0\n",
    "X_train = X_train.astype(np.int32)\n",
    "print(X_train[0])\n",
    "print(X_train[-1])\n",
    "print(Y_train)\n",
    "data = np.loadtxt('breast-cancer-wisconsin.data', dtype = 'str', delimiter=',', skiprows=599)\n",
    "X_test = data[:,:10]\n",
    "Y_test = data[:,10]\n",
    "X_test[X_test == '?'] = 0\n",
    "X_test = X_test.astype(np.int32)\n",
    "print(X_test[18])\n",
    "print(Y_test.shape)\n",
    "logistic_model = sklearn.linear_model.LogisticRegression(C = 35,fit_intercept=False, penalty='l1', solver='liblinear',max_iter = 1000)\n",
    "logistic_model.fit(X_train, Y_train);\n",
    "Y_pred = logistic_model.predict(X_test)\n",
    "print(Y_pred)\n",
    "sklearn.metrics.accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.000e+00 2.208e+01 1.146e+01 2.000e+00 4.000e+00 4.000e+00 1.585e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 1.000e+00 2.000e+00 1.000e+02 1.213e+03]\n",
      "[ 1.   37.33  6.5   2.    7.    8.    4.25  1.    1.   12.    1.    2.\n",
      " 93.    1.  ]\n",
      "[0.000e+00 2.075e+01 9.540e+00 2.000e+00 3.000e+00 4.000e+00 4.000e-02\n",
      " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 2.000e+00 2.000e+02 1.001e+03]\n",
      "(90,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8222222222222222"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('australian.dat',dtype = 'str', delimiter=' ', max_rows = 600)\n",
    "X_train = data[:,:14]\n",
    "Y_train = data[:,14]\n",
    "X_train[X_train == '?'] = 0\n",
    "X_train = X_train.astype(np.float)\n",
    "print(X_train[0])\n",
    "print(X_train[-1])\n",
    "data = np.loadtxt('australian.dat', dtype = 'str', delimiter=' ', skiprows=600)\n",
    "X_test = data[:,:14]\n",
    "Y_test = data[:,14]\n",
    "X_test[X_test == '?'] = 0\n",
    "X_test = X_test.astype(np.float)\n",
    "print(X_test[0])\n",
    "print(Y_test.shape)\n",
    "logistic_model = sklearn.linear_model.LogisticRegression(C = 35,fit_intercept=False, penalty='l2', solver='lbfgs',max_iter = 1000)\n",
    "logistic_model.fit(X_train, Y_train);\n",
    "Y_pred = logistic_model.predict(X_test)\n",
    "sklearn.metrics.accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  6.  4. 12.  5.  5.  3.  4.  1. 67.  3.  2.  1.  2.  1.  0.  0.  1.\n",
      "  0.  0.  1.  0.  0.  1.]\n",
      "[ 1. 18.  2. 42.  1.  3.  3.  3.  3. 42.  3.  1.  1.  1.  1.  0.  0.  0.\n",
      "  1.  0.  1.  0.  0.  1.]\n",
      "[ 1. 16.  4. 26.  1.  5.  3.  4.  2. 43.  1.  1.  1.  2.  1.  1.  0.  0.\n",
      "  0.  1.  0.  0.  0.  1.]\n",
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('german.data-numeric',dtype = 'str', max_rows = 900)\n",
    "X_train = data[:,:24]\n",
    "Y_train = data[:,24]\n",
    "X_train[X_train == '?'] = 0\n",
    "X_train = X_train.astype(np.float)\n",
    "print(X_train[0])\n",
    "print(X_train[-1])\n",
    "data = np.loadtxt('german.data-numeric', dtype = 'str', skiprows=900)\n",
    "X_test = data[:,:24]\n",
    "Y_test = data[:,24]\n",
    "X_test[X_test == '?'] = 0\n",
    "X_test = X_test.astype(np.float)\n",
    "print(X_test[0])\n",
    "print(Y_test.shape)\n",
    "logistic_model = sklearn.linear_model.LogisticRegression(C = 35,fit_intercept=False, penalty='l2', solver='lbfgs',max_iter = 1000)\n",
    "logistic_model.fit(X_train, Y_train);\n",
    "Y_pred = logistic_model.predict(X_test)\n",
    "sklearn.metrics.accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.05000e+02  8.13000e+02  2.73908e+05  2.73912e+05  1.60000e+01\n",
      "  8.00000e+00  4.00000e+00  2.12100e+03  1.14000e+02  1.48000e+02\n",
      "  1.35800e+03  0.00000e+00  1.00000e+00  5.00000e+01  8.02600e-01\n",
      "  5.00000e-01  5.00000e-01  5.90000e-03  1.00000e+00  1.00000e+00\n",
      "  0.00000e+00  1.20410e+00  9.03100e-01  6.02100e-01 -5.00000e-01\n",
      "  3.56000e-02  1.41700e-01]\n",
      "[ 1.06400e+03  1.07900e+03  4.06616e+05  4.06673e+05  4.19000e+02\n",
      "  4.00000e+01  6.00000e+01  4.46090e+04  9.70000e+01  1.19000e+02\n",
      "  1.35300e+03  0.00000e+00  1.00000e+00  7.00000e+01  4.05000e-01\n",
      "  5.09900e-01  2.63200e-01  1.11000e-02  3.75000e-01  9.50000e-01\n",
      "  1.00000e+00  2.62220e+00  1.17610e+00  1.75590e+00  7.36800e-01\n",
      " -1.68200e-01  9.75500e-01]\n",
      "[ 9.710000e+02  9.790000e+02  1.246698e+06  1.246717e+06  1.090000e+02\n",
      "  1.500000e+01  1.900000e+01  1.221200e+04  1.000000e+02  1.270000e+02\n",
      "  1.694000e+03  0.000000e+00  1.000000e+00  7.000000e+01  8.442000e-01\n",
      "  2.829000e-01  4.211000e-01  4.700000e-03  5.333000e-01  1.000000e+00\n",
      "  1.000000e+00  2.037400e+00  9.031000e-01  1.278700e+00  5.789000e-01\n",
      " -1.247000e-01  2.686000e-01]\n",
      "(141,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46808510638297873"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "data = np.loadtxt('Faults(1).NNA')\n",
    "# print( data[data[:,27] > 0][:,27] = 2)\n",
    "c1 = data[data[:,27] > 0] \n",
    "c2 = data[data[:,28] > 0] \n",
    "c3 = data[data[:,29] > 0] \n",
    "c4 = data[data[:,30] > 0] \n",
    "c5 = data[data[:,31] > 0] \n",
    "c6 = data[data[:,32] > 0] \n",
    "c7 = data[data[:,33] > 0] \n",
    "c2[:,27] = 2\n",
    "c3[:,27] = 3\n",
    "c4[:,27] = 4\n",
    "c5[:,27] = 5\n",
    "c6[:,27] = 6\n",
    "c7[:,27] = 7\n",
    "data = np.concatenate((c1,c2,c3,c4,c5,c6,c7))\n",
    "\n",
    "p = np.random.permutation(data.shape[0])\n",
    "data = data[p]\n",
    "data = data[:,:28]\n",
    "data1 = data[:1800,:]\n",
    "data2 = data[1800:,:]\n",
    "X_train = data1[:,:27]\n",
    "Y_train = data1[:,27]\n",
    "print(X_train[0])\n",
    "print(X_train[-1])\n",
    "# data = np.loadtxt('german.data-numeric', dtype = 'str', skiprows=900)\n",
    "X_test = data2[:,:27]\n",
    "Y_test = data2[:,27]\n",
    "print(X_test[0])\n",
    "print(Y_test.shape)\n",
    "logistic_model = sklearn.linear_model.LogisticRegression(C = 10,fit_intercept=True, penalty='l2', \n",
    "                 solver='saga',multi_class = 'multinomial',max_iter = 10000)\n",
    "logistic_model.fit(X_train, Y_train);\n",
    "Y_pred = logistic_model.predict(X_test)\n",
    "sklearn.metrics.accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[3.9000e+01 7.0000e+00 7.7516e+04 9.0000e+00 1.3000e+01 4.0000e+00\n",
      " 1.0000e+00 1.0000e+00 4.0000e+00 1.0000e+00 2.1740e+03 0.0000e+00\n",
      " 4.0000e+01 3.9000e+01 0.0000e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8035548110924995"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('adult.data',dtype = 'str',delimiter=',')\n",
    "# data[data == ' ?'] = 0\n",
    "print(data.shape[1])\n",
    "columns = np.arange(15)\n",
    "df = pd.DataFrame(data,columns=columns)\n",
    "\n",
    "df = MultiColumnLabelEncoder(columns = [1,3,5,6,7,8,9,13,14]).fit_transform(df)\n",
    "data = df.values\n",
    "data = data.astype(np.float)\n",
    "print(data[0])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    data[:,:14],data[:,14], test_size=0.33, random_state=42)\n",
    "logistic_model = sklearn.linear_model.LogisticRegression(C = 35,fit_intercept=False, penalty='l2', solver='lbfgs',max_iter = 1000)\n",
    "logistic_model.fit(X_train, Y_train);\n",
    "Y_pred = logistic_model.predict(X_test)\n",
    "sklearn.metrics.accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 6. 6. ... 4. 7. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/encs/pkg/anaconda3-2019.07/root/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6040816326530613"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('yeast.data',dtype = 'str')\n",
    "columns = np.arange(10)\n",
    "df = pd.DataFrame(data,columns=columns)\n",
    "df = MultiColumnLabelEncoder(columns = [0,9]).fit_transform(df)\n",
    "data = df.values\n",
    "data = data.astype(np.float)\n",
    "print(data[:,9])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    data[:,:9],data[:,9], test_size=0.33, random_state=42)\n",
    "logistic_model = sklearn.linear_model.LogisticRegression(C = 10,fit_intercept=True, penalty='none', \n",
    "                 solver='lbfgs',multi_class = 'multinomial',max_iter = 10000)\n",
    "logistic_model.fit(X_train, Y_train);\n",
    "Y_pred = logistic_model.predict(X_test)\n",
    "sklearn.metrics.accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0     1     2     3  4  5  6  7  8     9  10 11 12 13 14  15 16\n",
      "0    DGN2  2.88  2.16  PRZ1  F  F  F  T  T  OC14  F  F  F  T  F  60  F\n",
      "1    DGN3   3.4  1.88  PRZ0  F  F  F  F  F  OC12  F  F  F  T  F  51  F\n",
      "2    DGN3  2.76  2.08  PRZ1  F  F  F  T  F  OC11  F  F  F  T  F  59  F\n",
      "3    DGN3  3.68  3.04  PRZ0  F  F  F  F  F  OC11  F  F  F  F  F  54  F\n",
      "4    DGN3  2.44  0.96  PRZ2  F  T  F  T  T  OC11  F  F  F  T  F  73  T\n",
      "5    DGN3  2.48  1.88  PRZ1  F  F  F  T  F  OC11  F  F  F  F  F  51  F\n",
      "6    DGN3  4.36  3.28  PRZ1  F  F  F  T  F  OC12  T  F  F  T  F  59  T\n",
      "7    DGN2  3.19   2.5  PRZ1  F  F  F  T  F  OC11  F  F  T  T  F  66  T\n",
      "8    DGN3  3.16  2.64  PRZ2  F  F  F  T  T  OC11  F  F  F  T  F  68  F\n",
      "9    DGN3  2.32  2.16  PRZ1  F  F  F  T  F  OC11  F  F  F  T  F  54  F\n",
      "10   DGN3  2.56  2.32  PRZ0  F  T  F  T  F  OC12  F  F  F  F  F  60  F\n",
      "11   DGN3  4.28  4.44  PRZ1  F  F  F  F  F  OC12  F  F  F  T  F  58  F\n",
      "12   DGN3     3  2.36  PRZ1  F  F  F  T  T  OC11  F  F  F  T  F  68  F\n",
      "13   DGN2  3.98  3.06  PRZ2  F  F  F  T  T  OC14  F  F  F  T  F  80  T\n",
      "14   DGN3  1.96   1.4  PRZ1  F  F  F  T  F  OC11  F  F  F  T  F  77  F\n",
      "15   DGN3  4.68  4.16  PRZ1  F  F  F  T  F  OC12  F  F  F  T  F  62  F\n",
      "16   DGN2  2.21  1.88  PRZ0  F  T  F  F  F  OC12  F  F  F  T  F  56  F\n",
      "17   DGN2  2.96  1.67  PRZ0  F  F  F  F  F  OC12  F  F  F  T  F  61  F\n",
      "18   DGN3   2.6  1.68  PRZ1  F  F  F  T  F  OC12  F  F  F  T  F  70  F\n",
      "19   DGN3  2.88  2.48  PRZ0  F  F  F  F  F  OC11  F  F  F  T  F  71  F\n",
      "20   DGN3  4.48  3.48  PRZ0  F  F  F  F  F  OC12  F  F  F  T  F  51  F\n",
      "21   DGN4  3.32  2.84  PRZ0  F  F  F  F  F  OC12  F  F  F  T  F  62  F\n",
      "22   DGN3  2.36  1.68  PRZ0  F  F  F  F  F  OC12  F  F  F  T  F  62  F\n",
      "23   DGN3  3.68  2.32  PRZ0  F  F  F  F  F  OC11  F  F  F  T  F  62  F\n",
      "24   DGN8  4.32   3.2  PRZ0  F  F  F  F  F  OC11  F  F  F  F  F  58  T\n",
      "25   DGN5  4.56  72.8  PRZ0  T  T  F  T  F  OC12  F  F  F  T  F  57  F\n",
      "26   DGN3  3.24  3.08  PRZ1  F  F  F  T  F  OC11  F  F  F  T  F  60  F\n",
      "27   DGN3   3.4  3.06  PRZ1  F  F  F  T  T  OC11  F  F  F  T  F  68  T\n",
      "28   DGN3  3.16  2.69  PRZ1  F  F  F  T  T  OC11  F  F  F  T  F  56  F\n",
      "29   DGN6  3.96  3.28  PRZ0  F  F  F  F  F  OC11  F  F  F  T  F  61  F\n",
      "..    ...   ...   ...   ... .. .. .. .. ..   ... .. .. .. .. ..  .. ..\n",
      "440  DGN3     3  2.44  PRZ1  F  F  F  T  T  OC12  F  F  F  T  F  65  F\n",
      "441  DGN2  4.44  3.64  PRZ0  F  F  F  F  F  OC12  F  F  F  F  F  62  F\n",
      "442  DGN2  4.08  2.24  PRZ1  F  F  T  T  F  OC12  F  F  F  F  F  61  F\n",
      "443  DGN3  4.12   3.2  PRZ2  F  F  F  T  T  OC11  F  F  F  F  F  76  F\n",
      "444  DGN3  2.56  60.9  PRZ0  F  F  F  F  F  OC11  F  F  F  T  F  50  F\n",
      "445  DGN3  2.72  1.76  PRZ0  F  F  F  F  F  OC11  F  F  F  T  F  63  F\n",
      "446  DGN8   5.2   4.1  PRZ0  F  F  F  F  F  OC12  F  F  F  F  F  49  F\n",
      "447  DGN2   4.4  3.72  PRZ1  F  F  F  T  T  OC12  F  F  F  T  F  52  F\n",
      "448  DGN3  2.96  2.24  PRZ0  F  F  F  F  F  OC12  F  F  F  T  F  69  F\n",
      "449  DGN3  2.84  1.88  PRZ1  F  F  F  T  F  OC12  F  F  F  T  F  53  T\n",
      "450  DGN3  2.28  1.68  PRZ2  F  F  F  T  T  OC11  F  F  F  F  F  77  F\n",
      "451  DGN4  3.04  2.36  PRZ1  F  F  F  T  F  OC12  F  F  F  T  F  59  F\n",
      "452  DGN3   2.8  2.24  PRZ1  T  F  F  T  F  OC13  F  F  F  T  F  70  F\n",
      "453  DGN3  2.84  2.32  PRZ1  F  T  F  T  T  OC11  F  F  F  T  F  72  F\n",
      "454  DGN3  3.24  2.76  PRZ0  F  F  F  F  F  OC11  F  F  F  T  F  70  F\n",
      "455  DGN4  2.92  1.92  PRZ1  F  F  F  T  F  OC12  F  F  F  T  F  70  F\n",
      "456  DGN3   2.4  1.24  PRZ1  F  F  F  T  F  OC12  F  F  F  T  F  62  F\n",
      "457  DGN3  4.56   3.2  PRZ0  F  F  F  T  F  OC11  F  F  F  T  F  61  F\n",
      "458  DGN3   3.6     3  PRZ1  F  F  F  T  F  OC11  F  F  F  F  F  46  F\n",
      "459  DGN3  4.28  3.16  PRZ0  F  F  F  T  F  OC12  F  F  F  F  F  66  F\n",
      "460  DGN4  4.65  3.78  PRZ1  F  F  F  T  F  OC12  F  F  F  F  F  55  F\n",
      "461  DGN3  1.84  1.56  PRZ1  T  T  F  T  F  OC12  F  F  F  F  F  72  F\n",
      "462  DGN3  2.12  1.68  PRZ2  T  T  F  F  F  OC11  F  F  F  T  F  74  F\n",
      "463  DGN4  3.44  2.16  PRZ1  F  F  F  T  T  OC12  T  F  F  T  F  57  T\n",
      "464  DGN3  3.08  2.16  PRZ1  F  F  F  T  T  OC13  F  F  F  T  F  79  F\n",
      "465  DGN2  3.88  2.12  PRZ1  F  F  F  T  F  OC13  F  F  F  T  F  63  F\n",
      "466  DGN3  3.76  3.12  PRZ0  F  F  F  F  F  OC11  F  F  F  T  F  61  F\n",
      "467  DGN3  3.04  2.08  PRZ1  F  F  F  T  F  OC13  F  F  F  F  F  52  F\n",
      "468  DGN3  1.96  1.68  PRZ1  F  F  F  T  T  OC12  F  F  F  T  F  79  F\n",
      "469  DGN3  4.72  3.56  PRZ0  F  F  F  F  F  OC12  F  F  F  T  F  51  F\n",
      "\n",
      "[470 rows x 17 columns]\n",
      "     0     1     2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
      "0     1  2.88  2.16   1   0   0   0   1   1   3   0   0   0   1   0  60   0\n",
      "1     2   3.4  1.88   0   0   0   0   0   0   1   0   0   0   1   0  51   0\n",
      "2     2  2.76  2.08   1   0   0   0   1   0   0   0   0   0   1   0  59   0\n",
      "3     2  3.68  3.04   0   0   0   0   0   0   0   0   0   0   0   0  54   0\n",
      "4     2  2.44  0.96   2   0   1   0   1   1   0   0   0   0   1   0  73   1\n",
      "5     2  2.48  1.88   1   0   0   0   1   0   0   0   0   0   0   0  51   0\n",
      "6     2  4.36  3.28   1   0   0   0   1   0   1   1   0   0   1   0  59   1\n",
      "7     1  3.19   2.5   1   0   0   0   1   0   0   0   0   1   1   0  66   1\n",
      "8     2  3.16  2.64   2   0   0   0   1   1   0   0   0   0   1   0  68   0\n",
      "9     2  2.32  2.16   1   0   0   0   1   0   0   0   0   0   1   0  54   0\n",
      "10    2  2.56  2.32   0   0   1   0   1   0   1   0   0   0   0   0  60   0\n",
      "11    2  4.28  4.44   1   0   0   0   0   0   1   0   0   0   1   0  58   0\n",
      "12    2     3  2.36   1   0   0   0   1   1   0   0   0   0   1   0  68   0\n",
      "13    1  3.98  3.06   2   0   0   0   1   1   3   0   0   0   1   0  80   1\n",
      "14    2  1.96   1.4   1   0   0   0   1   0   0   0   0   0   1   0  77   0\n",
      "15    2  4.68  4.16   1   0   0   0   1   0   1   0   0   0   1   0  62   0\n",
      "16    1  2.21  1.88   0   0   1   0   0   0   1   0   0   0   1   0  56   0\n",
      "17    1  2.96  1.67   0   0   0   0   0   0   1   0   0   0   1   0  61   0\n",
      "18    2   2.6  1.68   1   0   0   0   1   0   1   0   0   0   1   0  70   0\n",
      "19    2  2.88  2.48   0   0   0   0   0   0   0   0   0   0   1   0  71   0\n",
      "20    2  4.48  3.48   0   0   0   0   0   0   1   0   0   0   1   0  51   0\n",
      "21    3  3.32  2.84   0   0   0   0   0   0   1   0   0   0   1   0  62   0\n",
      "22    2  2.36  1.68   0   0   0   0   0   0   1   0   0   0   1   0  62   0\n",
      "23    2  3.68  2.32   0   0   0   0   0   0   0   0   0   0   1   0  62   0\n",
      "24    6  4.32   3.2   0   0   0   0   0   0   0   0   0   0   0   0  58   1\n",
      "25    4  4.56  72.8   0   1   1   0   1   0   1   0   0   0   1   0  57   0\n",
      "26    2  3.24  3.08   1   0   0   0   1   0   0   0   0   0   1   0  60   0\n",
      "27    2   3.4  3.06   1   0   0   0   1   1   0   0   0   0   1   0  68   1\n",
      "28    2  3.16  2.69   1   0   0   0   1   1   0   0   0   0   1   0  56   0\n",
      "29    5  3.96  3.28   0   0   0   0   0   0   0   0   0   0   1   0  61   0\n",
      "..   ..   ...   ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
      "440   2     3  2.44   1   0   0   0   1   1   1   0   0   0   1   0  65   0\n",
      "441   1  4.44  3.64   0   0   0   0   0   0   1   0   0   0   0   0  62   0\n",
      "442   1  4.08  2.24   1   0   0   1   1   0   1   0   0   0   0   0  61   0\n",
      "443   2  4.12   3.2   2   0   0   0   1   1   0   0   0   0   0   0  76   0\n",
      "444   2  2.56  60.9   0   0   0   0   0   0   0   0   0   0   1   0  50   0\n",
      "445   2  2.72  1.76   0   0   0   0   0   0   0   0   0   0   1   0  63   0\n",
      "446   6   5.2   4.1   0   0   0   0   0   0   1   0   0   0   0   0  49   0\n",
      "447   1   4.4  3.72   1   0   0   0   1   1   1   0   0   0   1   0  52   0\n",
      "448   2  2.96  2.24   0   0   0   0   0   0   1   0   0   0   1   0  69   0\n",
      "449   2  2.84  1.88   1   0   0   0   1   0   1   0   0   0   1   0  53   1\n",
      "450   2  2.28  1.68   2   0   0   0   1   1   0   0   0   0   0   0  77   0\n",
      "451   3  3.04  2.36   1   0   0   0   1   0   1   0   0   0   1   0  59   0\n",
      "452   2   2.8  2.24   1   1   0   0   1   0   2   0   0   0   1   0  70   0\n",
      "453   2  2.84  2.32   1   0   1   0   1   1   0   0   0   0   1   0  72   0\n",
      "454   2  3.24  2.76   0   0   0   0   0   0   0   0   0   0   1   0  70   0\n",
      "455   3  2.92  1.92   1   0   0   0   1   0   1   0   0   0   1   0  70   0\n",
      "456   2   2.4  1.24   1   0   0   0   1   0   1   0   0   0   1   0  62   0\n",
      "457   2  4.56   3.2   0   0   0   0   1   0   0   0   0   0   1   0  61   0\n",
      "458   2   3.6     3   1   0   0   0   1   0   0   0   0   0   0   0  46   0\n",
      "459   2  4.28  3.16   0   0   0   0   1   0   1   0   0   0   0   0  66   0\n",
      "460   3  4.65  3.78   1   0   0   0   1   0   1   0   0   0   0   0  55   0\n",
      "461   2  1.84  1.56   1   1   1   0   1   0   1   0   0   0   0   0  72   0\n",
      "462   2  2.12  1.68   2   1   1   0   0   0   0   0   0   0   1   0  74   0\n",
      "463   3  3.44  2.16   1   0   0   0   1   1   1   1   0   0   1   0  57   1\n",
      "464   2  3.08  2.16   1   0   0   0   1   1   2   0   0   0   1   0  79   0\n",
      "465   1  3.88  2.12   1   0   0   0   1   0   2   0   0   0   1   0  63   0\n",
      "466   2  3.76  3.12   0   0   0   0   0   0   0   0   0   0   1   0  61   0\n",
      "467   2  3.04  2.08   1   0   0   0   1   0   2   0   0   0   0   0  52   0\n",
      "468   2  1.96  1.68   1   0   0   0   1   1   1   0   0   0   1   0  79   0\n",
      "469   2  4.72  3.56   0   0   0   0   0   0   1   0   0   0   1   0  51   0\n",
      "\n",
      "[470 rows x 17 columns]\n",
      "[3. 1. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 3. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 0. 1. 0. 0. 0. 0. 3. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 2. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      " 0. 0. 0. 3. 0. 1. 1. 1. 1. 3. 1. 0. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1.\n",
      " 0. 1. 2. 0. 1. 1. 0. 0. 1. 0. 0. 1. 2. 0. 1. 0. 2. 3. 1. 1. 0. 1. 0. 1.\n",
      " 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 2. 1. 1. 0. 1. 0. 1. 1. 3. 1. 0. 0.\n",
      " 0. 2. 1. 1. 1. 0. 0. 1. 3. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 1. 0. 2. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1.\n",
      " 1. 1. 0. 0. 2. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1.\n",
      " 0. 1. 3. 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 3. 1. 0. 0.\n",
      " 0. 1. 1. 2. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 3. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 2. 1. 1. 1. 1. 1. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 3. 1. 0. 1. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 3. 0. 2. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 3. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0.\n",
      " 1. 1. 0. 1. 2. 3. 1. 1. 2. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 0. 1. 2. 0. 1. 0. 0. 0. 1. 1. 1. 0. 3. 1. 1. 1. 1. 2. 0. 1. 3. 1. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 2. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 1. 0. 1. 2. 2. 0. 2. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('ThoraricSurgery.arff',dtype = 'str',delimiter=',',comments='@')\n",
    "columns = np.arange(17)\n",
    "df = pd.DataFrame(data,columns=columns)\n",
    "print(df)\n",
    "df = MultiColumnLabelEncoder(columns = [0,3,4,5,6,7,8,9,10,11,12,13,14,16]).fit_transform(df)\n",
    "print(df)\n",
    "data = df.values\n",
    "data = data.astype(np.float)\n",
    "print(data[:,9])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    data[:,:16],data[:,16], test_size=0.33, random_state=42)\n",
    "logistic_model = sklearn.linear_model.LogisticRegression(C = 35,fit_intercept=False, penalty='l2', solver='lbfgs',max_iter = 1000)\n",
    "logistic_model.fit(X_train, Y_train);\n",
    "Y_pred = logistic_model.predict(X_test)\n",
    "sklearn.metrics.accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9050410316529894"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('seismic-bumps.arff',dtype = 'str',delimiter=',',comments=('@','%'))\n",
    "columns = np.arange(19)\n",
    "df = pd.DataFrame(data,columns=columns)\n",
    "df = MultiColumnLabelEncoder(columns = [0,1,2,7]).fit_transform(df)\n",
    "data = df.values\n",
    "data = data.astype(np.float)\n",
    "print(data[:,9])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    data[:,:18],data[:,18], test_size=0.33, random_state=42)\n",
    "logistic_model = sklearn.linear_model.LogisticRegression(C = 35,fit_intercept=False, penalty='l2', solver='lbfgs',max_iter = 1000)\n",
    "logistic_model.fit(X_train, Y_train);\n",
    "Y_pred = logistic_model.predict(X_test)\n",
    "sklearn.metrics.accuracy_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35577917360388955"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('winequality-red.csv',delimiter=';',skiprows=1)\n",
    "scaler = sklearn.preprocessing.StandardScaler().fit(data[:,:11])\n",
    "X_scaled = scaler.transform(data[:,:11])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    data[:,:11],data[:,11], test_size=0.33, random_state=42)\n",
    "linear_model = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "linear_model.fit(X_train, Y_train)\n",
    "linear_model.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.710969075080917"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('communities.data',delimiter=',',dtype = 'str',)\n",
    "data = data[:,5:]\n",
    "data[data == '?'] = 0\n",
    "data = data.astype(np.float)\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler().fit(data[:,:11])\n",
    "X_scaled = scaler.transform(data[:,:11])\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    data[:,:122],data[:,122], test_size=0.33, random_state=42)\n",
    "linear_model = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "linear_model.fit(X_train, Y_train)\n",
    "linear_model.score(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(395, 33)\n",
      "['\"GP\"' '\"F\"' '18' '\"U\"' '\"GT3\"' '\"A\"' '4' '4' '\"at_home\"' '\"teacher\"'\n",
      " '\"course\"' '\"mother\"' '2' '2' '0' '\"yes\"' '\"no\"' '\"no\"' '\"no\"' '\"yes\"'\n",
      " '\"yes\"' '\"no\"' '\"no\"' '4' '3' '4' '1' '1' '3' '6' '\"5\"' '\"6\"' '6']\n",
      "(649, 33)\n",
      "4\n",
      "(1044, 13)\n",
      "(369, 33)\n"
     ]
    }
   ],
   "source": [
    "# np.unique([1, 1, 2, 2, 3, 3])\n",
    "\n",
    "# a = np.array([[1, 4], [2, 3]])\n",
    "# print(a)\n",
    "# np.unique(a)\n",
    "# data = np.array([[1,8,3,3,4],\n",
    "#                  [1,8,9,9,4],\n",
    "#                  [1,8,3,3,4]])\n",
    "# unique_rows,indices = np.unique(data, axis=0,return_index = True)\n",
    "# print(data[indices])\n",
    "\n",
    "data = np.loadtxt('student-mat.csv',delimiter=';',dtype = 'str',skiprows=1)\n",
    "print(data.shape)\n",
    "print(data[0])\n",
    "data1 =  np.loadtxt('student-por.csv',delimiter=';',dtype = 'str',skiprows=1)\n",
    "print(data1.shape)\n",
    "data = np.concatenate((data,data1))\n",
    "print(data[0,23])\n",
    "\n",
    "dupli = np.hstack((data[:,:11],data[:,19].reshape(-1,1),data[:,21].reshape(-1,1)))\n",
    "print(dupli.shape)\n",
    "unique_rows,countindices = np.unique(dupli, axis=0,return_counts = True)\n",
    "i, = np.where(countindices > 1)\n",
    "mergedData = data[i]\n",
    "print(mergedData.shape)\n",
    "\n",
    "\n",
    "# a = np.array([1, 2, 6, 4, 2, 3, 2])\n",
    "# u, indices = np.unique(a, return_counts=True)\n",
    "\n",
    "# print(indices)\n",
    "# # ind = indices[indices > 1]\n",
    "# i, = np.where(indices > 1)\n",
    "# print(i)\n",
    "# print(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# def find_kth(numbers , k):\n",
    "#     numbers.sort()\n",
    "#     j = 0\n",
    "#     for(i in numbers.length):\n",
    "#         if(j==k):\n",
    "#             return k\n",
    "#         if(!(numbers[i]==numbers[j])):\n",
    "#             j++\n",
    "            \n",
    "            \n",
    "# find_kth([1,1,2,3,4],2)\n",
    "\n",
    "        \n",
    "str = \"1010111101\"\n",
    "i = 0\n",
    "count = 0\n",
    "for s in str:\n",
    "#     print(str.count(\"101\",i,3))\n",
    "    if(str.count(\"101\",i,i+3) > 0):\n",
    "        count = count + 1\n",
    "    i = i + 1\n",
    "\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
